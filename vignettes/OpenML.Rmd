---
title: "OpenML"
author: "The OpenML R Team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{OpenML}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r init, include=FALSE}
# library("knitr")
# opts_chunk$set(cache = TRUE)
library("OpenML")
setOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f")
```

# Introduction {#intro}
The R package OpenML is an interface to make interactions with the [OpenML](http://openml.org/) server as comfortable as possible.
Users can download and upload files, run their implementations on specific tasks, get predictions in the correct form, and run SQL queries, etc. directly via R commands.
In this tutorial, we will show you the most important functions of this package and give you examples on standard workflows.

For general information on what OpenML is, please have a look at the [README file](https://github.com/openml/OpenML/blob/master/README.md) or visit the [official OpenML website](http://openml.org/).

Before making practical use of the package, in most cases it is desirable to [setup a configuration file](#config) to simplify further steps.
Afterwards, there are different basic stages when using this package or OpenML, respectively:

* [Listing](#listing)
    * function names begin with `listOML`
    * result is always a `data.frame`
    * available for `DataSets`, `Tasks`, `Flows`, `Runs`, `RunEvaluations`, `EvaluationMeasures` and `TaskTypes`
* [Downloading](#download)
    * function names begin with `getOML`
    * result is an object of a specific OpenML class
    * available for `DataSets`, `Tasks`, `Runs`, `Predictions` and `Flows`
* [Running models on tasks](#running)
    * function `runTaskMlr`
    * input: `OMLTask` and [`Learner`](https://mlr-org.github.io/mlr-tutorial/release/html/learner/index.html)
    * output: `OMLMlrRun`, `OMLRun`
* [Uploading](#upload)
    * function `uploadOMLRun`


# Configuration {#config}
## Registration
The first step of working with OpenML should be to register yourself at the [OpenML website](http://www.openml.org).
Most of the package's functions require a API authentication key which is only accessible with a (free) account.
To access the API key

* log into your account
* and then go to <http://www.openml.org/u#!api>.

For demonstration purposes, we have created a public, read-only API key (`"c1994bdb7ecb3c6f3c8f3b35f4b47f1f"`) which will be used in the following to make the examples executable.

## Permanently setting configuration
After registration you should create a configuration file.
The `config` file may contain the following information:

* `apikey`:
    * required to access the server
* `server`:
    * default: `http://www.openml.org`
* `verbosity`:
    * `0`: normal output
    * `1`: info output (default)
    * `2`: debug output
* `cachedir`:
    * directory where current cache contents are stored; the default cache directory can be obtained
      by the R command `file.path(tempdir(), "cache")`.
* `arff.reader`:
    * `RWeka`: This is the standard Java parser used in Weka.
    * `farff`: The [farff package](http://www.github.com/mlr-org/farff) lives below the mlr-org and is a newer, faster parser without any Java requirements.

The configuration file is not mandatory.
Yet, permanently setting your API key via a `config` file is recommended, as this key is required to access the OpenML server.
Note however, that basically everybody who has access to your computer can read the configuration file and see your API key.
With your API key other users have full access to your account via the API, so please handle the API key with care.

The configuration file and some related things are also explained in the [OpenML Wiki](https://github.com/openml/OpenML/wiki/Client-API).

### Creating the configuration file in R
You can easily create a configuration file using the command `saveOMLConfig`.
To create a configuration file using default values and setting your API key, use
```{r, eval = FALSE}
library(OpenML)
saveOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f")
```
where `"c1994bdb7ecb3c6f3c8f3b35f4b47f1f"` should be replaced with your personal API key.

### Manually creating the configuration file
Alternatively, you can manually create a file `~/.openml/config` in your home directory (you can use the R command `path.expand("~/.openml/config")` to get the full path to the configuration file on your operating system).
The `config` file consists of `key = value` pairs. An exemplary minimal `config` file might look as follows:
```{r eval = FALSE}
apikey=c1994bdb7ecb3c6f3c8f3b35f4b47f1f
```
Note that values are not quoted.

If one manually modifies the `config` file  one needs to reload the modified `config` file to your current R session using `loadOMLConfig()`.
To query the current configuration, one can use
```{r}
library(OpenML)
getOMLConfig()
```

### Temporarily changing the configuration
If you want to modify your configuration only for the current R session and without changing the `config` file, you can use `setOMLConfig()` with the same key/value pairs as above.

If you have done these steps, you are ready to go!



# Listing {#listing}
In this stage, we want to list basic information about the various OpenML objects such as data sets, tasks, flows, runs, run results, evaluation measures or task types.
See the [OpenML introduction](http://openml.org/guide) for an overview on and explanations of the different objects.

For each of these objects we have a function to query the information beginning with `listOML`. All of these functions return a `data.frame`, even when the result has only one column.

First, load the package:
```{r results="hide"}
library("OpenML")
setOMLConfig(verbosity = 0) # switch off status output
```

### List data sets
To browse the OpenML data base for appropriate data sets, you can use `listOMLDataSets()` in order to get basic data characteristics (number of features/instances/classes/missing values etc.) for each data set.
By default, `listOMLDataSets()` returns only data sets that have an active status on OpenML.
If you need data sets that are either `"in_preparation"` or `"deactivated"`, you can change the `status` parameter:
```{r}
datasets = listOMLDataSets()  # returns active data sets
names(datasets)
head(datasets[, 1:6])

inactive.data = listOMLDataSets(status = "deactivated")  # returns deactivated data sets
head(inactive.data[, 1:6])
```

To find a specific data set, one can now query the resulting `datasets` object. Suppose we want
to find the `iris` data set.
```{r}
subset(datasets, name == "iris")
```
As one can see there are two data sets called `iris`.
We want to use the original data set with three classes.
It has the data set ID `did` = `r subset(datasets, name == "iris" & NumberOfClasses == 3)$did`.
One can also look at the data set on the OpenML web page http://openml.org/d/`r subset(datasets, name == "iris" & NumberOfClasses == 3)$did`.

### List tasks
Each OpenML task is a bundle of a data set, a target feature, a (performance) estimation procedure (e.g., 10-fold CV), data splits for this estimation procedure and, finally, one or more (performance) evaluation measures.
Every task has a type, e.g., `"Supervised Classification"` or `"Supervised Regression"`.
To list tasks one can use
```{r}
tasks = listOMLTasks()
names(tasks)
head(tasks[, 1:5])
```
For some data sets, there may be more than one task available at the OpenML server.
For example, you can look for `"Supervised Classification"` tasks that are available for a specific data set as follows:
```{r}
# subset tasks to "Supervised Classification" for the iris data (did == 61)
head(subset(tasks, task.type == "Supervised Classification" & did == 61L)[, 1:5])
```

### List flows
A flow is the definition and implementation of a specific algorithm workflow or script.
I.e., a flow is essentially the code that implements the algorithm.
```{r}
flows = listOMLFlows()
names(flows)
flows[56:63, 1:2]
```

### List runs and run results
A run is a combination of a setup (**??? FIXME??? Setup is not explained (ans why should one use it if it cannot be queried?**) and a task.
The results are stored as a run result.
Both, runs and run results can be listed.
Here one has additional arguments to subset the result.
For example one can search for a specific `task.id`, `setup.id` or `implementation.id`.
To list all runs for [task 59](http://www.openml.org/t/59) (supervised classification on iris) one can use
```{r}
runs = listOMLRuns(task.id = 59L)  # must be restricted to a task, setup and/or implementation ID
head(runs)

# FIXME: This does not work in the vignette?
# runresults = listOMLRunEvaluations(task.id = 61)  # a task ID must be supplied
# colnames(runresults)
```

### List evaluation measures and task types
To list further objects such as evaluation measures and task types one can simply use the respective functions.
```{r}
listOMLEvaluationMeasures()
listOMLTaskTypes()
```



# Downloading {#download}
Users can download data, tasks, flows and runs from the OpenML server.
The package provides special representations for each object which will be discussed here.

## Download an OpenML data set only
To directly download a data set, e.g., when you want to run a few preliminary experiments, one can use the function `getOMLDataSet`.
The function accepts a data set ID as input and returns the corresponding `OMLDataSet`:
```{r}
iris.data = getOMLDataSet(did = 61L)  # the iris data set has the data set ID 61
iris.data
```

## Download an OpenML task
The following call returns an OpenML task object for a supervised classification task on the iris data:
```{r}
task = getOMLTask(task.id = 59L)
task
```
The corresponding `"OMLDataSet"` object can be accessed by
```{r}
task$input$data.set
```
A special print function gives the basic information on the data set.
To extract the data itself one can use
```{r}
iris.data = task$input$data.set$data
head(iris.data)
```


## Download an OpenML flow
You can download a flow by specifying the `flow.id` parameter in the `getOMLFlow` function:
```{r}
flow = getOMLFlow(flow.id = 2700L)
flow
```

## Download an OpenML run
To download the results of one run including all server and user computed metrics, you have to know the corresponding run ID.
These IDs can be extracted from the `runs` object, which was created in the previous section.
Here we use the first run of task 59, which has the `run.id` 234.
You can download a single OpenML run with the `getOMLRun` function:
```{r}
run = getOMLRun(run.id = 525534L)  # see ?OMLRun for each slot of the OMLRun object
```
There are some slots which are of major interest for the `OMLRun` object.
A list containing the parameter settings can be obtained by `run$parameter.setting`:
```{r}
run$parameter.setting  ## retrieve first parameter setting
```
(**??? FIXME: WHAT DOES THIS REPRESENT???**)
The first three parameters refer to the RNG settings, e.g. the seed, which will help you to reproduce the run.
It the flow that created this run has hyperparameters that are different from the default values of the corresponding learner, they are also shown, otherwise the default hyperparameters are used.

All data that served as input for the run, including the URL to the data is stored in
```{r}
run$input.data
```
(**??? FIXME: STRANGE???**)

To retrieve predictions of an uploaded run, you can use:
```{r}
head(run$predictions)
```

# Running {#running}
You can apply an implementation of an algorithm to a specific task. There are several possibilities to do this.

## Run a task with a specified mlr learner
If you are working with [**mlr**](https://github.com/mlr-org/mlr), you can specify a `RLearner` object and use the function `runTaskMlr` to create the desired `"OMLMlrRun"` object.
The `task` is created the same was as in the previous sections:
```{r, warning = FALSE, message = FALSE}
task = getOMLTask(task.id = 59L)

library(mlr)
lrn = makeLearner("classif.rpart")
run.mlr = runTaskMlr(task, lrn)
run.mlr
```
Note that locally created runs don't have run ID or flow id yet.
These are assigned by the OpenML server after uploading the run.

## Run a task without using mlr
If you are not using **mlr**, you will have to invest quite a bit more time to get things done. So -- unless you have good reasons to do otherwise -- we strongly encourage you to use **mlr**.

The following example shows how to create an OpenML flow description object manually.

The first step is to create a list of `OMLFlowParameter`, where each parameter of your implementation is stored.
Let's assume we have written an algorithm that has two parameters called `a` (with default value: `500`) and `b` (with default value: `TRUE`).
```{r}
flow.par.a = makeOMLFlowParameter(
  name = "a",
  data.type = "numeric",
  default.value = "500",  # All defaults must be passed as strings.
  description = "An optional description of parameter a.")

flow.par.b = makeOMLFlowParameter(
  name = "b",
  data.type = "logical",
  default.value = "TRUE",
  description = "An optional description of parameter b.")

flow.pars = list(flow.par.a, flow.par.b)
```
Now we can create the whole description object.
Try to find a good name for your algorithm that gives other users an idea of what is happening.
```{r}
oml.flow = makeOMLFlow(
  name = "good_name",
  external.version = "1.0",
  description = "A proper description of your algorithm, changes, etc.",
  parameter = flow.pars)
```

Before you can apply the created flow to a task, you have to create a `OMLRun` object.
If you want to change the parameter settings for the run, you can do this by a list that contains an `OMLRunParameter` objects for each parameter defined by the flow **whose setting varies from the default**.
The class `OMLRunParameter` has the following slots:

* name
* value
* component (optional and only needed if the parameter belongs to a (sub-)component of the implementation. Then, the name of this component must be handed over here.) (**??? Fixme: What is a (sub-)component? Any good example here???**)

Let's assume that we want to set the parameter `a` to a value of `300`.
Parameter `b`, on the other hand, remains in the default setting, so that we do not need to define a `OMLRunParameter` for it:
```{r}
run.par.a = makeOMLRunParameter(name = "a", value = "300")
run.pars = list(run.par.a)
```
Now you can create your `OMLRun` object using the `makeOMLRun` function.
If you want to upload the run to OpenML you will need to create a `data.frame` for the `predictions` parameter, which has to be in a standardized form before you can create a run using `makeOMLRun`.
The call `task$output$predictions` gives us the expected column names and their types.
For supervised classification and regression tasks, these are:

```{r}
str(task$output$predictions$features)
```
The columns `repeat`, `fold` and `row_id` have to be zero-based, i.e., we start numbering with 0 and not with 1 as we would usually do in R.
For an example, see below.

Additionally, in case of a classification task one needs:

* confidence.*classname_1*
* confidence.*classname_2*
* ...

I.e., one column for each level of the target variable.


### Example: An excerpt of predictions (Iris data set, 2x10-fold CV).

Here we used two repetitions (`repeat` can be either `0` or `1`) of a 10-fold cross-validation (`fold` can be in `0:9`). The data set has 150 observations, thus, the `row_id` can be in `0:149`.

(**??? Fixme: This is created WHILE or AFTER running the learner? I.e. `prediction` and `confidence.xyz` are the results of the learner? And how would one usually create all these objects? I.e. are the run parameters perhaps the result from cross-validation?**)

```{r}
set.seed(1907)  # set seed for reproducible results
preds = data.frame('repeat' = rep(0:1, each = nrow(iris.data)),
                   fold = rep(0:9, each = nrow(iris.data)/10),
                   row.id = sample(1:nrow(iris.data)) - 1)
names(preds)[1] = "repeat"
head(preds)
```
(**??? Fixme: Why can't one specify `repeat = ...`**)

```
    repeat fold row_id      prediction confidence.Iris-setosa confidence.Iris-versicolor confidence.Iris-virginica
1        0    0    140  Iris-virginica                      0                          0                         1
...    ...  ...    ...             ...                    ...                        ...                       ...
51       0    3     37     Iris-setosa                      1                          0                         0
...    ...  ...    ...             ...                    ...                        ...                       ...
150      0    9     76  Iris-virginica                      0                          0                         1
151      1    0    110  Iris-virginica                      0                          0                         1
...    ...  ...    ...             ...                    ...                        ...                       ...
300      1    9     58 Iris-versicolor                      0                          1                         0
```

(**??? Fixme: Is it possible to NOT wrap the above code block but use scroll bars instead? This would strongly increase readability. Problem would be the PDF version, though.**)

When you have created such a data frame you can create an OpenML run by:

```{r eval = FALSE}
run = makeOMLRun(task.id = 59L, parameter.setting = run.pars, predictions = preds)
```
This run can now be uploaded to the [OpenML server](http://openml.org) as we will show in the next section.



# Uploading {#upload}

## Upload a flow using mlr

(**??? Fixme: Shoudln't we explain first, why we need to upload the implementation before uploading the results? And isn't that needed only once?**)

A flows is an implementation of single algorithms or scripts.
To create an flow, we can use the `mlr` package.
Each `mlr` learner can be considered as an implementation of a flow that can be uploaded to the server with the function `uploadOMLFlow`.
If the flow has already been uploaded to the server, we get a message that the flow already exists and the `flow.id` is returned from the function.
Otherwise, the not existing flow is uploaded and a new `flow.id` is assigned to it.

```{r eval = TRUE, warning = FALSE, message = TRUE}
library(mlr)
lrn = makeLearner("classif.randomForest")
flow.id = uploadOMLFlow(lrn)
flow.id
```
(**??? Fixme: What happens if a flow already exists on the server? And what happens here if we use the read-only API key? The tutorial states that we get a message that the flow already exists. This is not the case!**)

## Upload a flow without using mlr

(**??? Fixme: We don't want to do this with the sourcefile anymore. Should we provide a link how to create a mlr-learner as alternative?**)

In the previous section, we explained how to create an `OMLFlow` manually and created the object `oml.flow`, which reflects the description object of the flow. Before you can upload this flow to the server, you have to write an R-script containing the algorithm you want to use as flow. Let's assume you have done this and have a string `sourcefile` containing the path to your R-script. Your flow can now be uploaded as follows:

```{r eval = FALSE}
oml.flow.id = uploadOMLFlow(oml.flow, sourcefile = sourcefile)
oml.flow.id
```

## Upload an OpenML run to the server

Runs that have been created using `mlr` can be uploaded by:
```{r eval = FALSE, warning = FALSE, message = TRUE}
run.id = uploadOMLRun(run.mlr)
```
Before the run is uploaded, `uploadOMLRun` is looking if the flow that created this run is available on the server.
If the flow is not available on the server, it is automatically uploaded.
(**??? Fixme: Cannot be executed with read-only API key. I still would like to execute this in order to check the code...**)


# Example workflow with mlr {#workflow}

Here we will show you how a standard workflow could look like. This just a small example, to get further information about the package and its functions, please have a look at the other sections of this tutorial or the [detailed documentation](http://www.rdocumentation.org/packages/OpenML).

For this example, let's assume we wanted to test how good the R implementation of `lda` (from package **MASS**) is, in comparison with all uploaded runs on 20 randomly selected tasks.
We only want to consider tasks that have less than 10 features, not more than 100 instances, exactly 2 classes in the target feature and not a single missing value.

**FIXME: Move lengthy comments to Markdown text**

```{r}
set.seed(2315)
library(mlr)

tl = listOMLTasks()

# Find tasks that match our specifications
task.ids = subset(tl, task.type == "Supervised Classification",
  NumberOfFeatures < 10 & NumberOfFeatures > 3 &
  NumberOfInstances < 100 & NumberOfClasses == 2 &
  NumberOfMissingValues == 0, select = task.id, drop = TRUE)
length(task.ids)

# Randomly select 20 of them
task.ids = sample(task.ids, 20)
```
Next, we create and upload the learner, compute predictions and upload these.

```{r eval = FALSE}
# create the mlr learner for lda:
lrn = makeLearner("classif.lda")

# Upload the implementation and retrieve its implementation ID with
implementation.id = uploadOMLFlow(lrn)

run.ids = c()
for (id in task.ids) {
  task = getOMLTask(id)
  res = try(runTaskMlr(task, lrn)) # try to compute predictions with our learner
  run.id = uploadOMLRun(res, implementation.id = implementation.id, session.hash = hash)
  run.ids = c(run.ids, run.id)
}
```

**FIXME: Check if flow/task/... already exists on server? Download run results?**

Now, we compute the quantiles of the measure `"predictive.accuracy"` of all runs using our `lda` implementation in comparison to the measures of different implementations.
Quantiles close to one correspond to "lda has achieved (one of) the best results", quantiles close to zero correspond to "the other flows were better".

```{r boxplot_pred_accuracy, eval = FALSE}
qs = c()
for (id in task.ids) {
  metrics = listOMLRunResults(id)
  if (is.null(metrics$predictive.accuracy)){
    cat("skip")
    next
  }
  f = ecdf(metrics$predictive.accuracy)
  q = f(metrics[metrics$implementation.id == implementation.id, "predictive.accuracy"])
  qs = c(qs, q)
}

boxplot(qs, ylim = c(0, 1), main = "Quantiles of lda measures")
```
As we can see in the boxplot, the performance of `lda` varies quite strongly. Mostly, though, the `lda` measures were better than the average of all run results on the considered tasks.
**FIXME: Really? It looks like the meadian is approx. 0.5**
