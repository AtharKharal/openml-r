---
title: "OpenML"
author: "The OpenML R Team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{OpenML}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r init, include=FALSE}
# library("knitr")
# opts_chunk$set(cache = TRUE)
library("OpenML")
setOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f")
```

## Introduction {#intro}
The R package OpenML is an interface to make interactions with the [OpenML](http://openml.org/) server as comfortable as possible.
Users can download and upload files, run their implementations on specific tasks, get predictions in the correct form, and run SQL queries, etc. directly via R commands.
In this tutorial, we will show the most important functions of this package and give examples on standard workflows.

For general information on what OpenML is, please have a look at the [README file](https://github.com/openml/OpenML/blob/master/README.md) or visit the [official OpenML website](http://openml.org/).

Before making practical use of the package, in most cases it is desirable to [setup a configuration file](#config) to simplify further steps.
Afterwards, there are different basic stages when using this package or OpenML, respectively:

* [Listing](#listing)
    * function names begin with `listOML`
    * result is always a `data.frame`
    * available for `DataSets`, `Tasks`, `Flows`, `Runs`, `RunEvaluations`, `EvaluationMeasures` and `TaskTypes`
* [Downloading](#download)
    * function names begin with `getOML`
    * result is an object of a specific OpenML class
    * available for `DataSets`, `Tasks`, `Runs`, `Predictions` and `Flows`
* [Running models on tasks](#running)
    * function `runTaskMlr`
    * input: `OMLTask` and [`Learner`](https://mlr-org.github.io/mlr-tutorial/release/html/learner/index.html)
    * output: `OMLMlrRun`, `OMLRun`
* [Uploading](#upload)
    * function `uploadOMLRun`

### Private key notification

All examples in this tutorial are given with a <span style="color:red">**READ-ONLY key**</span>. 

With this key you can **read** all the information from the server but not **write** data sets, tasks, flows and runs to the server. This key allows to emulate uploading to the server but doesn't allow to really store data. If one wants to write data to a server, one has to **get your own personal key**. The process how to obtain a key is shown in the [configuration section](#config).

<span style="color:red">**Important: Please do not write meaningless data to a server such as copies of already existing data sets, duplicate tasks or runs (such as the ones from this tutorial)! One instance of the iris data set should be enough for everyone :D**</span>

### Basic example {#basicexample}

Below you can find an example that downloads a task from the server, presents some information about a task and uploads a new task to the server. 

```{r eval=TRUE, message=FALSE}
library("OpenML")
## temporarily set API key to read only key (replace it with your own key) 
setOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f")

# download a task 
task = getOMLTask(task.id = 1L) 
task
```

The task contains information on the following:

* task type: defines the type of a task (regression, classification, clustering, etc.)
* data set: which data set belongs to the given task (one task can always only be connected to a single data set)
* target feature(s): optional field for all kinds of classification and regression tasks 
* tags: tags / labels, which might be helpful for further subselections
* estimation procedure: which estimation procedure has been used when computing the performance

In the next line `randomForest` is used as a classifier and run with the help of the [`mlr package`](https://github.com/mlr-org/mlr). One needs to run the algorithm locally on your own computer. Note that `mlr` will automatically download the package that is needed to run the specified classifier.

```{r eval=FALSE}
# define the classifier (usually called "flow" within OpenML)
library(mlr)
lrn = makeLearner("classif.randomForest")

# upload the new flow (with information about the algorithm and settings);
# if this algorithm already exists on the server, one will receive a message with the ID of the existing flow
flow.id = uploadOMLFlow(lrn)

# the last step is to perform a run and upload the results
run.mlr = runTaskMlr(task, lrn)
run.id = uploadOMLRun(run.mlr)
```

Following this very brief example, we will explain the single steps of the OpenML package in more detail in the next sections.

## Configuration {#config}
### Registration
The first step of working with OpenML should be to register yourself at the [OpenML website](http://www.openml.org).
Most of the package's functions require an API authentication key, which is only accessible with a (free) account.
In order to receive your own API key {#personalapikey}

* go to the [OpenML website](http://www.openml.org) and log into your account
* then go to <http://www.openml.org/u#!api>.

For *demonstration purposes*, we have created a *public read-only API key* (`"c1994bdb7ecb3c6f3c8f3b35f4b47f1f"`), which will be used in the following to make the examples executable.

### Permanently setting configuration {#permanentconfig}
After registering, you should [create a configuration file](#createconfig). The `config` file may contain the following information:

* `server`:
    * default: `http://www.openml.org/api/v1`
* `cachedir`:
    * directory where current cache contents are stored
    * the default cache directory can be obtained by the R command `file.path(tempdir(), "cache")`.
* `verbosity`:
    * `0`: normal output
    * `1`: info output (default)
    * `2`: debug output
* `arff.reader`:
    * `RWeka`: this is the standard Java parser used in Weka
    * `farff`: the [farff package](http://www.github.com/mlr-org/farff) lives below the mlr-org and is a newer, faster parser without any Java requirements
* `apikey`:
    * required to access the server

The configuration file is *not mandatory*. Yet, permanently setting your API key via a `config` file is recommended, as this key is required to access the OpenML server. However, it is noteworthy that basically everybody who has access to your computer can read the configuration file and thus see your API key.
With your API key other users have full access to your account via the API, so please handle the API key with care!

The configuration file and some related things are also explained in the [OpenML Wiki](https://github.com/openml/OpenML/wiki/Client-API).

### Creating the configuration file in R {#createconfig}
One can easily create a configuration file using either `setOMLConfig` or `saveOMLConfig`. The difference between those two commands is that `setOMLConfig` sets your configuration temporarily for the current R session, whereas `saveOMLConfig` saves the configuration permanently.
In order to create a permanent configuration file using default values and at the same time setting your personal API key, call
```{r, eval = FALSE}
library(OpenML)
saveOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f")
```
where `"c1994bdb7ecb3c6f3c8f3b35f4b47f1f"` should be replaced with [*your personal API key*](#personalapikey).

### Manually creating the configuration file
It is also possible to manually create a file `~/.openml/config` in your home directory -- one can use the R command `path.expand("~/.openml/config")` to get the full path to the configuration file on the operating system.
The `config` file consists of `key = value` pairs. An exemplary minimal `config` file might look as follows:
```{r eval = FALSE}
apikey=c1994bdb7ecb3c6f3c8f3b35f4b47f1f
```
Note that the values are not quoted.

If one manually modifies the `config` file, one needs to reload the modified `config` file to the current R session using `loadOMLConfig()`. One can query the current configuration using
```{r}
library(OpenML)
getOMLConfig()
```

As one can see, the configuration file lists the five items (`server`, `cachedir`, `verbosity`, `arff.reader` and `apikey`) from [above](#permanentconfig).

Once the config file is set up, you are **ready to go**!

## Listing {#listing}
In this stage, we want to list basic information about the various OpenML objects:

* data sets
* tasks
* flows
* runs
* run results
* evaluation measures
* task types

See the [OpenML introduction](http://openml.org/guide) for a detailed overview on (and explanations of) the different objects.

For each of these objects we have a function to query the information beginning with `listOML`. All of these functions return a `data.frame`, even in case the result consists of a single column or have zero observations (i.e. rows).

One should be aware of the fact, that the `listOML*` functions only list information on the corresponding objects -- they do not download the respective objects. Information on actually [downloading](#download) specific objects is covered within the next section.

First, load the package:
```{r results="hide"}
library("OpenML")
setOMLConfig(verbosity = 0) # switch off status output
```

### List data sets
To browse the OpenML data base for appropriate data sets, you can use `listOMLDataSets()` in order to get basic data characteristics (number of features, instances, classes, missing values, etc.) for each data set.
By default, `listOMLDataSets()` returns only data sets that have an active status on OpenML.
If you need data sets that are either `"in_preparation"` or `"deactivated"`, you can change the `status` parameter accordingly:
```{r}
datasets = listOMLDataSets()  # returns active data sets
str(datasets)
head(datasets[, 1:5])

inactive.data = listOMLDataSets(status = "deactivated")  # returns deactivated data sets
head(inactive.data[, 1:5])
```

To find a specific data set, one can now query the resulting `datasets` object. Suppose we want
to find the `iris` data set.
```{r}
subset(datasets, name == "iris")
```

As one can see there are two data sets called `iris`. We want to use the original data set with three classes, which is stored under the data set ID (`did`) `r subset(datasets, name == "iris" & NumberOfClasses == 3)$did`.
One can also have a closer look at the data set on the OpenML web page http://openml.org/d/`r subset(datasets, name == "iris" & NumberOfClasses == 3)$did`.

### List tasks
Each OpenML task is a bundle that encapsulates information on various objects:

* the data set itself
* the target feature
* a (performance) estimation procedure,  e.g., a 10-fold cross-validation
* data splits for this estimation procedure
* one or more (performance) evaluation measures
* a specific type, e.g., `"Supervised Classification"` or `"Supervised Regression"`

Listing the tasks can be done via
```{r}
tasks = listOMLTasks()
str(tasks)
head(tasks[, 1:5])
```

For some data sets, there may be more than one task available on the OpenML server.
For example, you can look for `"Supervised Classification"` tasks that are available for a specific data set as follows:
```{r}
# subset tasks to "Supervised Classification" for the iris data (did == 61)
head(subset(tasks, task.type == "Supervised Classification" & did == 61L)[, 1:5])
```

### List flows
A flow is the definition and implementation of a specific algorithm workflow or script, i.e., a flow is essentially the code that implements the algorithm.
```{r}
flows = listOMLFlows()
str(flows)
flows[56:63, 1:4]
```

### List runs and run results
A run is an experiment, which is executed on a given combination of task, flow and setup. The corresponding results are stored as a run result. (##FIXME: Setup does not exist any more..)
Both objects, i.e., runs and run results, can be listed via `listOMLRuns` and `listOMLRunEvaluations`.
As each of those objects is defined with a task, setup and flow, one can extract runs and run results with specific combinations of `task.id`, `setup.id` and/or `flow.id`.
For instance, listing all runs for [task 59](http://www.openml.org/t/59) (supervised classification on iris) can be done with
```{r eval}
runs = listOMLRuns(task.id = 59L)  # must be restricted to a task, setup and/or implementation ID
head(runs)

run.results = listOMLRunEvaluations(task.id = 59L)  # a task ID must be supplied
str(run.results)
```

### List evaluation measures and task types
Analogously to the previous listings, one can list further objects simply by calling the respective functions.
```{r eval = FALSE}
listOMLDataSetQualities()
listOMLEstimationProcedures()
listOMLEvaluationMeasures()
listOMLTaskTypes()
```

## Downloading {#download}
Users can download data, tasks, flows and runs from the OpenML server.
The package provides special representations for each object, which will be discussed here.

### Download an OpenML data set
To directly download a data set, e.g., when you want to run a few preliminary experiments, one can use the function `getOMLDataSet`.
The function accepts a data set ID as input and returns the corresponding `OMLDataSet`:
```{r eval=FALSE}
iris.data = getOMLDataSet(did = 61L)  # the iris data set has the data set ID 61
```

### Download an OpenML task
The following call returns an OpenML task object for a supervised classification task on the iris data:
```{r eval=TRUE}
task = getOMLTask(task.id = 59L)
task
```

The corresponding `"OMLDataSet"` object can be accessed by
```{r eval=TRUE}
task$input$data.set
```

The class of the task can be shown with the next line
```{r eval=TRUE}
task$task.type
```

A special print function gives the basic information on the data set.
To extract the data itself one can use
```{r eval=TRUE}
iris.data = task$input$data.set$data
head(iris.data)
```

### Download an OpenML flow
You can download a flow by specifying the `flow.id` parameter in the `getOMLFlow` function:
```{r eval=TRUE}
flow = getOMLFlow(flow.id = 2700L)
flow
```

### Download an OpenML run
To download the results of one run including all server and user computed metrics, you have to know the corresponding run ID.
IDs for all runs related to the task can be extracted from the `runs` object, which was created in the previous section.
Here we use a run of task 59, which has the `run.id` 525534.
You can download a single OpenML run with the `getOMLRun` function:
```{r eval=FALSE}
run = getOMLRun(run.id = 525534L)
```
There are some arguments that are of major interest for the `OMLRun` object.
A list containing Random Number Generator (RNG) settings for reproducing a run and non-default settings for hyperparameters can be obtained by `run$parameter.setting`:
```{r eval=FALSE}
run$parameter.setting  # retrieve the list of parameter settings
```
(**??? FIXME: CHECK THE OUTPUT???**)
The first three parameters refer to the RNG settings, e.g. the seed or the type of the RNG, which will help you to reproduce the run.
It the flow that created this run has hyperparameters that are different from the default values of the corresponding learner, they are also shown, otherwise the default hyperparameters are used.

All data that served as input for the run, including data set(s) ID(s) and the URL to the data is stored in
```{r eval=FALSE}
run$input.data 
```
(**??? FIXME: CHECK IF IT IS CLEAR FOR USERS???**)

To retrieve predictions made by an uploaded run, you can use:
```{r eval=FALSE}
run$predictions # shows predicitons, ground truth information about classes and task-specific information, e.g. about confidence of a classifier for every data point or in which fold a data point has been placed 
```

## Running {#running}
The modularized structure of OpenML allows to apply the implementation of an algorithm to a specific task and there exist multiple possibilities to do this.

### Run a task with a specified mlr learner
If one is working with [**mlr**](https://github.com/mlr-org/mlr), one can specify a `RLearner` object and use the function `runTaskMlr` to create the desired `"OMLMlrRun"` object.
The `task` is created the same way as in the previous sections:
```{r, warning = FALSE, message = FALSE}
task = getOMLTask(task.id = 59L)

library(mlr)
lrn = makeLearner("classif.rpart")
run.mlr = runTaskMlr(task, lrn)
run.mlr
```
Note that locally created runs don't have a run ID or flow ID yet. These are assigned by the OpenML server after uploading the run.

### Run a task without using mlr
If one is not using **mlr**, one will have to invest quite a bit more time and effort to get things done. So, unless there are good reasons to do otherwise, we strongly encourage to use **mlr**.

The following example shows how to create an OpenML flow description object manually.

The first step is to create a list of `OMLFlowParameter`, where each parameter of your implementation is stored. Let's assume we have written an algorithm that has two parameters called `a` (with default value: `500`) and `b` (with default value: `TRUE`).
```{r}
flow.par.a = makeOMLFlowParameter(
  name = "a",
  data.type = "numeric",
  default.value = "500",  # All defaults must be passed as strings.
  description = "An optional description of parameter a.")

flow.par.b = makeOMLFlowParameter(
  name = "b",
  data.type = "logical",
  default.value = "TRUE",
  description = "An optional description of parameter b.")

flow.pars = list(flow.par.a, flow.par.b)
```
Now we can create the whole description object.
When creating a new flow, one should try to find a good name for the algorithm, so the other users get an idea of what is actually happening.
```{r}
oml.flow = makeOMLFlow(
  name = "good_name",
  external.version = "1.0",
  description = "A proper description of your algorithm, changes, etc.",
  parameter = flow.pars)
```

Before one can apply the created flow to a task, one has to create a `OMLRun` object.
If one wants to change the parameter settings for the run, one can do this by a list that contains an `OMLRunParameter` objects for each parameter defined by the flow **whose setting varies from the default**.
The class `OMLRunParameter` has the following slots:

* name
* value
* component (optional and only needed if the parameter belongs to a (sub-)component of the implementation. Then, the name of this component must be handed over here.) (**??? Fixme: What is a (sub-)component? Any good example here???**)

Let's assume that we want to set the parameter `a` to a value of `300`.
Parameter `b`, on the other hand, remains in the default setting, so that we do not need to define a `OMLRunParameter` for it:
```{r}
run.par.a = makeOMLRunParameter(name = "a", value = "300")
run.pars = list(run.par.a)
```
Now one can create the `OMLRun` object using the `makeOMLRun` function.
If one wants to upload the run to OpenML it is necessary to create a `data.frame` for the `predictions` parameter before actually creating the run with `makeOMLRun`. Note that the `data.frame` for the `predictions` has to be in a **standardized form**.
The call `task$output$predictions` returns the expected column names and their types.
For supervised classification and regression tasks, these are:

```{r}
str(task$output$predictions$features)
```
The columns `repeat`, `fold` and `row_id` have to be zero-based, i.e., we start numbering with 0 and not with 1 as we would usually do in R. There is an example in the next subsection.

Additionally, in case of a classification task, one needs:

* confidence.*classname_1*
* confidence.*classname_2*
* etc

i.e., one column for each level of the target variable.


### Example: An excerpt of predictions (Iris data set, 2x10-fold CV).

Now, we first show, how one could create the predictions and then, how to perform an OpenML run with these predictions. In the given scenario, we used two repetitions (`repeat` can be either `0` or `1`) of a 10-fold cross-validation (`fold` can be in `0:9`). The data set has 150 observations, thus, the `row_id` can be in `0:149`.

(**??? Fixme: This is created WHILE or AFTER running the learner? I.e. `prediction` and `confidence.xyz` are the results of the learner? And how would one usually create all these objects? I.e. are the run parameters perhaps the result from cross-validation?**)

```{r, message=FALSE}
task = getOMLTask(task.id = 59L)
iris.data = task$input$data.set$data

set.seed(1907)  # set seed for reproducible results
preds = data.frame(rep(0:1, each = nrow(iris.data)),
  rep(0:9, each = nrow(iris.data)/10),
  sample(1:nrow(iris.data)) - 1)
names(preds) = c("repeat", "fold", "row.id")
head(preds)
```
(**??? Fixme: Why can't one specify `repeat = ...`**)

```
    repeat fold row_id      prediction confidence.Iris-setosa   ... confidence.Iris-virginica
1        0    0    140  Iris-virginica                      0   ...                         1
...    ...  ...    ...             ...                    ...   ...                       ...
51       0    3     37     Iris-setosa                      1   ...                         0
...    ...  ...    ...             ...                    ...   ...                       ...
150      0    9     76  Iris-virginica                      0   ...                         1
151      1    0    110  Iris-virginica                      0   ...                         1
...    ...  ...    ...             ...                    ...   ...                       ...
300      1    9     58 Iris-versicolor                      0   ...                         0
```

(**??? Fixme: Is it possible to NOT wrap the above code block but use scroll bars instead? This would strongly increase readability. Problem would be the PDF version, though.**)

Once such a data frame has been created, one can create an OpenML run by calling:

```{r eval = FALSE}
run = makeOMLRun(task.id = 59L, parameter.setting = run.pars, predictions = preds)
```
This run can now be uploaded to the [OpenML server](http://openml.org) as we will show in the next section.

## Uploading {#upload}
The following section gives an overview on how one can contribute building blocks (i.e. data sets, flows and runs) to the OpenML server.

### Upload a data set 
A data set contains information that can be stored on OpenML and used by OpenML tasks and runs. In this example, a very simple (and admittedly rather stupid) data set is created, processed with `mlr` and uploaded to the OpenML server. The corresponding workflow of uploading a data sets consists of the following four steps:

1. `make*Task`: create a task (e.g. `makeClassifTask` for classification tasks)
2. `makeOMLDataSetDescription`: create the description object of an OpenML data set
3. `makeOMLDataSet`: convert this task to an OpenML data set
4. `uploadOMLDataSet`: upload the data set to the server

```{r eval = FALSE, warning = FALSE, message = TRUE} 

## This is a fake meaningless data set. Please DO NOT upload it to the server!
days = c(1, 10, 5)
startdate = c('2016-02-10', '2016-02-11', '2016-02-21')
task = c('Write code', 'Test', 'Make documentation')
project = data.frame(task, days, startdate)

## Create a new mlr classification task
sample.task = makeClassifTask(id = "my_project", data = project,
  target = "task", weights = NULL, blocking = NULL, positive = NA_character_,
  fixup.data = "warn", check.data = TRUE) 

## create a new OML data set from the mlr task
sampleOMLDataSet = makeOMLDataSet(desc = data.description, data = project, 
  colnames.old = colnames(project), colnames.new = colnames(project), 
  target.features = "task") 

(**FIXME: UPLOADING DOES NOT WORK WITH READ_ONLY KEY**)

## upload a data set to the server
dataset.id = uploadOMLDataSet(sample.task)
dataset.id
```

### Upload a flow using mlr

A flow is an implementation of a single algorithm or a script. To create a flow, we can use the `mlr` package, which already contains numerous implemented algorithms. Each `mlr` learner can be considered as an implementation of a flow, which then again can be uploaded to the server with the function `uploadOMLFlow`.
If the flow has already been uploaded to the server, one receives a message that the flow already exists and the `flow.id` is returned from the function.
Otherwise, the not existing flow is uploaded and a new `flow.id` is assigned to it.
 
```{r eval = FALSE, warning = FALSE, message = TRUE}
library(mlr)
lrn = makeLearner("classif.randomForest")
flow.id = uploadOMLFlow(lrn)
flow.id
```

### Upload a flow without using mlr

(**??? Fixme: We don't want to do this with the sourcefile anymore. Should we provide a link how to create a mlr-learner as alternative?**)
In the previous section, we explained how to create an `OMLFlow` manually and created the object `oml.flow`, which reflects the description object of the flow. Before one can upload the flow to the server, one has to store the algorithm (that should be used as a flow) within a single R-script. Assuming one has already done this step (and the path to the R-script is called `"path-to-sourcefile"`), one can can now upload the flow as follows:


```{r eval = FALSE}
oml.flow.id = uploadOMLFlow(oml.flow, sourcefile = "path-to-sourcefile")
oml.flow.id
```

### Upload an OpenML run to the server

In addition to uploading data sets or flows, one can also upload runs. Runs that have been created using `mlr` can be uploaded by:
```{r eval = FALSE, warning = FALSE, message = TRUE}
run.id = uploadOMLRun(run.mlr)
```
Before the run is uploaded, `uploadOMLRun` is checking whether the flow that created this run is available on the server. If the flow is not available on the server, it will also be uploaded (automatically).
(**??? Fixme: Cannot be executed with read-only API key. I still would like to execute this in order to check the code...**)
