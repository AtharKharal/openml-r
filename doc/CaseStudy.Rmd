---
title: "Example workflow using OpenML and mlr"
author: "The OpenML R Team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: Bib.bib
vignette: >
  %\VignetteIndexEntry{OpenML}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r init, include=FALSE}
# library("knitr")
# opts_chunk$set(cache = TRUE)
library("OpenML")
setOMLConfig(apikey = "c1994bdb7ecb3c6f3c8f3b35f4b47f1f")
```

In this vignette we illustrate the advantages of OpenML by performing a small comparison study between a random forest and bagged trees.
We first create the respective binary classification learners using mlr, then query OpenML for suitable data sets, apply the learners on the data, and finally evaluate the results.

# Create Learners {#learners}

Because there is a variety of tree implementations in R, we select three implementations of different algorithms. These are **rpart** from the package *rpart* [@rpart] as an implementation of *CART*, **J48** from the package *RWeka* [@RWeka] as an implementation of *C4.5*, and **ctree** from the package *party* [@party], which is an implementation of the algorithm *Conditional inference trees*. As for the *Random forest*, we use the implementation **ranger** from the package *ranger* [@ranger].

While the random forest learner can be used as-is, the trees can conveniently be combined using mlr's bagging wrapper.
The number of trees is set to 100 for both the forest and all bagged tree learners so that this parameter does not influence the results.

```{r prepare-case-study, include=FALSE}
library(mlr)
setOMLConfig(verbosity = 0)
```

```{r create-learners, cache=TRUE}
# create a random forest learner and three bagged tree learners
lrn1 = makeLearner("classif.ranger", num.trees = 100)
lrn2 = makeBaggingWrapper(makeLearner("classif.rpart"), bw.iters = 100)
lrn3 = makeBaggingWrapper(makeLearner("classif.J48"), bw.iters = 100)
lrn4 = makeBaggingWrapper(makeLearner("classif.ctree"), bw.iters = 100)
```

# Query OpenML

Now we search for appropriate tasks on OpenML by querying the server using `listOMLTasks()`, which returns a large data frame:
```{r get-tasks, cache=TRUE}
all.tasks = listOMLTasks()
dim(all.tasks)
```

For this study the candidates are filtered to meet the following criteria:  
1. Binary classification problem  
2. 10-fold Crossvalidation for validation  
3. No missing values -- Random Forest cannot handle them automatically  
4. Less than 1000 instances -- keep evaluation time low  
5. $n < p$  
6. Predictive accuracy as evaluation measure  

Although a data frame can be filtered with the `subset()` function (in R's base package), we strongly recommend the faster and more convenient alternatives provided by either **data.table** [@data.table] or **dplyr** [@dplyr].
```{r subset-task, seval=TRUE, cache=FALSE}
library(data.table)
tasks = as.data.table(all.tasks)
tasks = tasks[
  task.type == "Supervised Classification" &
  NumberOfClasses == 2 &
  estimation.procedure == "10-fold Crossvalidation" &
  NumberOfMissingValues == 0 &
  NumberOfInstances < 1000 &
  NumberOfNumericFeatures < NumberOfInstances &
  evaluation.measures == "predictive_accuracy", ]

nrow(tasks)
```
We randomly pick 20 out of the `r nrow(tasks)` remaining tasks to keep the runtimes reasonable.
Furthermore, we left join the data frame returned by `listOMLDataSets()` to have a quick glance at the names of the selected data sets:  
**Why not only call `tasks[, name]`?**  
```{r sample-tasks, cache=TRUE}
set.seed(52251)
tasks = tasks[sample(nrow(tasks), 20), ]
tasks[listOMLDataSets(), name, on = "did", nomatch = 0]
```

# Evaluation  

The function `runTaskMlr()` applies an mlr learner on an OpenML data set and returns a benchmark result from which we extract the aggregated performance measure:
```{r run-tasks, eval=FALSE}
runTask = function(task.id, learner.id) {
  # FIXME: See issue #145
  res = runTaskMlr(getOMLTask(task.id), learners[[learner.id]])
  getBMRAggrPerformances(res$mlr.benchmark.result)[[1]][[1]][1]
}
learners = list(lrn1, lrn2, lrn3, lrn4)
grid = expand.grid(task.id = tasks$task.id, learner.id = 1:4)
res = Map(runTask, task.id = grid$task.id, learner.id = grid$learner.id)
# runTask(tasks[1, task.id], lrn2)
```

**FIXME: better implement an OpenML version of benchmark and use mlr's plotting capabilities for benchmark results?**

In figure \ref{fig:diff}, the differences in predictive accuracy between each combination of two learners are visualized.
Besides boxplots, there are also so-called violin plots depicted in the background, that represent the estimated density of the differences' distributions.
![](figures/violinplot_binary.pdf)  

# References  
